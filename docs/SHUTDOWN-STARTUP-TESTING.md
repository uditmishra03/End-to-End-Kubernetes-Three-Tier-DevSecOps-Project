# Shutdown/Startup Testing and DNS Update Requirements

## Problem Identified

When running the shutdown ‚Üí startup workflow, **the ALB DNS hostname changes**, causing DNS records (hosted on **Hostinger**) to become invalid.

### What Happens:

1. **Shutdown (`shutdown-cluster.sh`):**
   - Deletes EKS node groups (worker nodes)
   - Backs up cluster state
   - **ALB is deleted** (managed by ALB Ingress Controller)

2. **Startup (`startup-cluster.sh`):**
   - Recreates EKS node groups
   - Deploys applications
   - **ALB Ingress Controller creates NEW ALB with NEW DNS**

3. **DNS Mismatch:**
   - Old records on Hostinger: `todo.tarang.cloud` ‚Üí `k8s-sharedalb-OLD123.us-east-1.elb.amazonaws.com`
   - New ALB: `k8s-sharedalb-NEW789.us-east-1.elb.amazonaws.com`
   - **Result: 404 errors or timeouts until DNS is manually updated**

---

## What Changes vs. What Persists

### ‚úÖ **Persists After Restart:**

| Component | Storage Location | Why It Persists |
|-----------|------------------|-----------------|
| **Grafana Dashboards** | 10Gi EBS PVC | Persistent volume claim |
| **Prometheus Metrics** | 20Gi EBS PVC | Persistent volume claim |
| **MongoDB Data** | EBS PVC | Persistent volume claim (if configured) |
| **Jenkins Config** | EC2 EBS Volume | Jenkins runs on EC2, not EKS |
| **SonarQube Data** | EC2 with Docker volumes | Runs on Jenkins server |
| **ECR Images** | AWS ECR | Managed service outside EKS |
| **EKS Control Plane** | AWS managed | Only nodes are deleted, not cluster |
| **IAM Roles/IRSA** | AWS IAM | Independent of cluster state |
| **Kubernetes Manifests** | Git repository | Version controlled |

### ‚ùå **Changes After Restart:**

| Component | Why It Changes | Impact |
|-----------|----------------|--------|
| **ALB DNS Hostname** | New ALB created by Ingress Controller | üî¥ **BREAKS DNS RECORDS** |
| **Worker Node IPs** | New EC2 instances | Minor - handled by k8s networking |
| **Pod IPs** | Pods recreated on new nodes | Minor - handled by Services |
| **Node Instance IDs** | New EC2 instances | Minor - EKS manages this |
| **ALB ARN** | New load balancer | Used by Route53/ACM |
| **Target Group ARNs** | New target groups | Managed by ALB controller |

---

## Solution: Manual DNS Update on Hostinger

**DNS Provider:** Hostinger (NOT AWS Route53)  
**Domain:** tarang.cloud  
**Update Method:** Manual update via Hostinger DNS panel

### **Why Manual Update is Required:**

The ALB DNS hostname is randomly generated by AWS and changes every time the ALB is recreated. Since DNS is managed on Hostinger (not AWS), there's no API integration between AWS and Hostinger to automatically update DNS records.

### **What the Startup Script Does:**

The `startup-cluster.sh` script has been updated to:
1. Detect the new ALB DNS hostname
2. **Display clear instructions** for manual DNS update on Hostinger
3. Show the exact CNAME values needed
4. Provide verification commands

**No automated DNS update** - you must manually update Hostinger DNS panel.

---

## Manual DNS Update Procedure

### **Step 1: Get New ALB DNS from Startup Script**

After running `./startup-cluster.sh`, the script will display:

```
‚ö†Ô∏è  IMPORTANT: ALB DNS HAS CHANGED!
==========================================

New ALB DNS: k8s-sharedalb-xyz789abc.us-east-1.elb.amazonaws.com

üìù Manual Update Required on Hostinger:
  1. Login to Hostinger DNS panel: https://hpanel.hostinger.com
  2. Navigate to: Domains ‚Üí tarang.cloud ‚Üí DNS Records
  ...
```

**Copy the ALB DNS hostname** - you'll need it for Step 2.

### **Step 2: Login to Hostinger**

1. Go to: https://hpanel.hostinger.com
2. Login with your credentials
3. Navigate to: **Domains** ‚Üí **tarang.cloud** ‚Üí **DNS / Name Servers** ‚Üí **DNS Records**

### **Step 3: Update todo.tarang.cloud CNAME**

1. Find the existing CNAME record for `todo.tarang.cloud`
2. Click **Edit** (pencil icon)
3. Update the **Points to** field with the new ALB DNS:
   ```
   k8s-sharedalb-xyz789abc.us-east-1.elb.amazonaws.com
   ```
4. Keep TTL at 300 seconds (or default)
5. Click **Save** or **Update**

### **Step 4: Update monitoring.tarang.cloud CNAME**

1. Find the existing CNAME record for `monitoring.tarang.cloud`
2. Click **Edit** (pencil icon)
3. Update the **Points to** field with the same new ALB DNS:
   ```
   k8s-sharedalb-xyz789abc.us-east-1.elb.amazonaws.com
   ```
4. Keep TTL at 300 seconds (or default)
5. Click **Save** or **Update**

### **Step 5: Wait for DNS Propagation**

- **Hostinger propagation:** 5-15 minutes
- **Global DNS propagation:** Up to 30 minutes (depends on TTL)

---

## Verification Steps

### **1. Check New ALB DNS (if needed later)**
```bash
# SSH to Jenkins server first
ssh -i your-key.pem ubuntu@<jenkins-ip>

# Get ALB DNS from Ingress
kubectl get ingress mainlb -n three-tier -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'

# Should return something like:
# k8s-sharedalb-xyz789.us-east-1.elb.amazonaws.com
```

### **2. Test DNS Resolution**
```bash
# Test application domain
dig +short todo.tarang.cloud

# Test monitoring domain  
dig +short monitoring.tarang.cloud

# Both should resolve to new ALB DNS
# If they show old DNS, wait a few more minutes for propagation
```

### **3. Test HTTPS Endpoints**
```bash
# Test application
curl -I https://todo.tarang.cloud/
# Expected: HTTP 200 or 301/302 (redirect)

# Test backend health
curl https://todo.tarang.cloud/healthz
# Expected: {"status":"Healthy"...}

# Test Grafana
curl -I https://monitoring.tarang.cloud/grafana
# Expected: HTTP 200 or 301/302

# Test Prometheus
curl -I https://monitoring.tarang.cloud/prometheus  
# Expected: HTTP 200 or 301/302
```

## Testing Workflow

### **Prerequisites:**
1. Jenkins EC2 instance must be running
2. Access to Hostinger DNS panel (https://hpanel.hostinger.com)
3. `kubectl` configured to access EKS cluster
4. Startup/shutdown scripts in place

### **Step-by-Step Test:**

#### **1. Run Shutdown Script**
```bash
# SSH to Jenkins server
ssh -i your-key.pem ubuntu@<jenkins-ip>

# Navigate to scripts directory
cd /path/to/End-to-End-Kubernetes-Three-Tier-DevSecOps-Project/scripts

# Run shutdown
./shutdown-cluster.sh
```

**Expected outcome:**
- Node groups deleted
- Pods terminated
- ALB deleted (automatically by ALB controller)
- Backup created in S3

**Time:** ~5-10 minutes

#### **2. Manually Stop Jenkins EC2**
```bash
# From AWS Console or AWS CLI
aws ec2 stop-instances --instance-ids i-<jenkins-instance-id> --region us-east-1
```

**Expected outcome:**
- Jenkins server stopped
- All services down
- Cost savings active

**Time:** ~2-3 minutes

#### **3. Start Jenkins EC2**
```bash
# From AWS Console or AWS CLI
aws ec2 start-instances --instance-ids i-<jenkins-instance-id> --region us-east-1

# Wait for instance to be running
aws ec2 wait instance-running --instance-ids i-<jenkins-instance-id> --region us-east-1
```

**Expected outcome:**
- Jenkins server running
- Jenkins UI accessible at `http://<jenkins-ip>:8080`
- SonarQube accessible at `http://<jenkins-ip>:9000`

**Time:** ~3-5 minutes

#### **4. Run Startup Script**
```bash
# SSH to Jenkins server
ssh -i your-key.pem ubuntu@<jenkins-ip>

# Navigate to scripts directory
cd /path/to/End-to-End-Kubernetes-Three-Tier-DevSecOps-Project/scripts

# Run startup
./startup-cluster.sh
```

**Expected outcome:**
- ‚úÖ Node groups created
- ‚úÖ Pods deployed (frontend, backend, MongoDB)
- ‚úÖ Monitoring pods running (Grafana, Prometheus)
- ‚úÖ ALB created with NEW DNS
- ‚úÖ **Script displays DNS update instructions with new ALB DNS**
- ‚ö†Ô∏è HTTPS endpoints NOT accessible yet (DNS still points to old ALB)

**Time:** ~10-15 minutes

#### **5. Update DNS on Hostinger (MANUAL)**

Follow the "Manual DNS Update Procedure" section above:
1. Copy new ALB DNS from startup script output
2. Login to Hostinger: https://hpanel.hostinger.com
3. Update CNAME for `todo.tarang.cloud`
4. Update CNAME for `monitoring.tarang.cloud`
5. Save changes

**Time:** ~2-3 minutes

#### **6. Wait for DNS Propagation**

```bash
# Check DNS resolution every few minutes
watch -n 30 'dig +short todo.tarang.cloud'

# Should eventually show new ALB DNS
```

**Time:** 5-30 minutes (depending on TTL and DNS cache)

#### **7. Verify Everything Works**

Once DNS has propagated:
- ‚úÖ Application accessible at `https://todo.tarang.cloud/`
- ‚úÖ Backend API working at `https://todo.tarang.cloud/api/tasks`
- ‚úÖ Grafana accessible at `https://monitoring.tarang.cloud/grafana`
- ‚úÖ Prometheus accessible at `https://monitoring.tarang.cloud/prometheus`
- ‚úÖ All dashboards and data intact (persistence verified)

---

## What to Expect During Testing

### ‚úÖ **Should Work Immediately After Startup:**
- ALB created with new DNS
- Pods running and healthy
- Internal cluster networking functional
- Jenkins/SonarQube accessible via EC2 IP
- `kubectl` commands work
- Direct ALB DNS works: `http://<new-alb-dns>/`

### ‚ö†Ô∏è **Won't Work Until DNS Updated:**
- `https://todo.tarang.cloud/` - Times out or 404
- `https://monitoring.tarang.cloud/grafana` - Times out or 404
- Any custom domain access

### ‚úÖ **After DNS Update on Hostinger:**
- All HTTPS custom domain URLs work
- SSL certificates valid
- Full application functionality restored

---

## Common Issues and Solutions

### **Issue 1: Can't Access Hostinger DNS Panel**

**Symptoms:**
- Forgot Hostinger login credentials
- Can't find DNS records section

**Solution:**
```
1. Password reset: https://hpanel.hostinger.com/password-reset
2. After login: Domains ‚Üí tarang.cloud ‚Üí DNS / Name Servers ‚Üí DNS Records
3. If still can't find: Contact Hostinger support
```

### **Issue 2: DNS Not Propagating**

**Symptoms:**
```bash
dig +short todo.tarang.cloud
# Still shows old ALB DNS after 30 minutes
```

**Causes:**
1. Browser/system DNS cache
2. ISP DNS cache
3. Wrong TTL value

**Solution:**
```bash
# Clear local DNS cache (Windows)
ipconfig /flushdns

# Clear local DNS cache (Linux/Mac)
sudo systemd-resolve --flush-caches  # Linux
sudo dscacheutil -flushcache  # Mac

# Use Google DNS for testing (bypasses ISP cache)
dig @8.8.8.8 +short todo.tarang.cloud

# Check from different location
# Use online DNS checker: https://dnschecker.org
```

**Symptoms:**
```
‚ö† ALB URL not found yet. May need a few more minutes.
```

**Causes:**
1. ALB Ingress Controller not running
2. IRSA (IAM Roles for Service Accounts) not configured
3. Ingress manifest errors

**Solution:**
```bash
# Check ALB controller pods
kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller

# Check ALB controller logs
kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=100

# Check ingress events
kubectl describe ingress mainlb -n three-tier

# Verify IRSA configuration
kubectl get sa aws-load-balancer-controller -n kube-system -o yaml | grep eks.amazonaws.com
```

### **Issue 3: Monitoring Pods Not Starting**

**Symptoms:**
```
prometheus-grafana-xxx    0/1     Pending
```

**Causes:**
1. PVC not binding to PV
2. EBS CSI driver not installed
3. Insufficient node resources

**Solution:**
```bash
# Check PVC status
kubectl get pvc -n monitoring

# Check PV status
kubectl get pv

# Check pod events
kubectl describe pod prometheus-grafana-xxx -n monitoring

# If EBS CSI driver missing:
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master"
```

### **Issue 4: 504 Gateway Timeout After Restart**

**Symptoms:**
```
curl https://todo.tarang.cloud/
# Returns: 504 Gateway Timeout
```

**Causes:**
1. ALB targets not healthy yet
2. Health check path incorrect
3. Pods still initializing

**Solution:**
```bash
# Check target group health
aws elbv2 describe-target-health \
  --target-group-arn <target-group-arn> \
  --region us-east-1

# Check pod readiness
kubectl get pods -n three-tier -o wide

# Check pod logs
kubectl logs -f <backend-pod-name> -n three-tier

# Wait 2-3 minutes for health checks to pass
```

---

## Alternative: Quick DNS Lookup Reference

If you need to get the new ALB DNS later (after closing the terminal):

```bash
# SSH to Jenkins server
ssh -i your-key.pem ubuntu@<jenkins-ip>

# Get current ALB DNS
kubectl get ingress mainlb -n three-tier -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
echo ""

# Show formatted instructions
echo "Update these DNS records on Hostinger:"
echo "  todo.tarang.cloud ‚Üí $(kubectl get ingress mainlb -n three-tier -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
echo "  monitoring.tarang.cloud ‚Üí $(kubectl get ingress mainlb -n three-tier -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
```

---

## Files Modified

1. **`scripts/startup-cluster.sh`** (MODIFIED)
   - Added Step 10.5: Display DNS update instructions
   - Shows new ALB DNS prominently
   - Provides step-by-step Hostinger update guide
   - Removed incorrect Route53 automation

2. **`docs/SHUTDOWN-STARTUP-TESTING.md`** (MODIFIED - this file)
   - Corrected DNS provider from Route53 to Hostinger
   - Added manual DNS update procedure for Hostinger
   - Updated verification steps
   - Removed automated DNS update references

