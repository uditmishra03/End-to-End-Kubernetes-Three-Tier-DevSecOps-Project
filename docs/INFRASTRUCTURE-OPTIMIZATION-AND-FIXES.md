# Infrastructure Optimization and Critical Fixes

## Document Overview
This document captures the significant infrastructure changes, performance optimizations, and critical issue resolutions implemented to stabilize and optimize the Kubernetes Three-Tier DevSecOps project.

**Date:** November 17, 2025  
**Status:** âœ… All Issues Resolved

---

## Table of Contents
1. [Jenkins Instance Upgrade](#1-jenkins-instance-upgrade)
2. [Jenkins JVM Configuration Optimization](#2-jenkins-jvm-configuration-optimization)
3. [SonarQube Persistent Storage Implementation](#3-sonarqube-persistent-storage-implementation)
4. [SonarQube Docker Restart Policy](#4-sonarqube-docker-restart-policy)
5. [ALB Health Check Configuration Fix](#5-alb-health-check-configuration-fix)
6. [504 Gateway Timeout Resolution](#6-504-gateway-timeout-resolution)

---

## 1. Jenkins Instance Upgrade

### Problem
Jenkins server (t2.2xlarge) was experiencing performance issues during concurrent builds, particularly when running SonarQube analysis alongside Docker image builds.

### Root Cause
- Insufficient compute resources for parallel pipeline execution
- High CPU utilization during SonarQube scans
- Memory pressure during npm install operations

### Solution Implemented
**Changed instance type from `t2.2xlarge` to `c6a.2xlarge`**

**Specifications Comparison:**
| Resource | t2.2xlarge | c6a.2xlarge | Improvement |
|----------|-----------|-------------|-------------|
| vCPUs | 8 | 8 | Same |
| Memory | 32 GB | 16 GB | -50% |
| CPU Type | Burstable | Compute-optimized | Better sustained performance |
| Network | Up to 5 Gbps | Up to 12.5 Gbps | +150% |
| Cost/hour | ~$0.3712 | ~$0.306 | -18% savings |

**Key Benefits:**
- âœ… Better sustained CPU performance (no throttling)
- âœ… 18% cost reduction ($2.20/day savings)
- âœ… Improved network throughput for Docker registry operations
- âœ… More consistent build times

**Implementation Steps:**
1. Stopped Jenkins EC2 instance
2. Changed instance type via AWS Console
3. Associated Elastic IP (54.82.232.211) to maintain static address
4. Restarted instance and verified Jenkins functionality

**Verification:**
```bash
# Instance details
aws ec2 describe-instances --instance-ids i-0d5f8c6a4b2e1f3a7 \
  --query 'Reservations[0].Instances[0].[InstanceType,State.Name,PublicIpAddress]'
```

---

## 2. Jenkins JVM Configuration Optimization

### Problem
Default Jenkins JVM settings were not optimized for the workload, causing:
- Frequent garbage collection pauses
- Out-of-memory errors during large builds
- Slow Jenkins UI responsiveness

### Solution Implemented
**Optimized JVM heap and garbage collection settings**

**Configuration Changes:**
```bash
# /etc/default/jenkins or systemd override
JAVA_OPTS="-Xms2048m -Xmx4096m -XX:+UseG1GC -XX:MaxGCPauseMillis=200"
```

**Parameters Explained:**
- `-Xms2048m`: Initial heap size (2GB)
- `-Xmx4096m`: Maximum heap size (4GB) - 25% of 16GB RAM
- `-XX:+UseG1GC`: Use G1 garbage collector (better for large heaps)
- `-XX:MaxGCPauseMillis=200`: Target max GC pause time

**Benefits:**
- âœ… Reduced GC pause times
- âœ… Better memory utilization
- âœ… Improved Jenkins responsiveness
- âœ… More headroom for concurrent builds

---

## 3. SonarQube Persistent Storage Implementation

### Problem
SonarQube container restarts resulted in complete data loss:
- Lost authentication tokens
- Lost project configurations
- Lost quality gates and webhooks
- Had to reconfigure everything after each restart

### Root Cause
SonarQube container was running without persistent volumes, storing all data inside the ephemeral container filesystem.

### Solution Implemented
**Added Docker volume mounts for SonarQube data persistence**

**Updated `tools-install.sh`:**
```bash
docker run -d \
  --name sonar \
  -p 9000:9000 \
  -v /opt/sonarqube/data:/opt/sonarqube/data \
  -v /opt/sonarqube/logs:/opt/sonarqube/logs \
  -v /opt/sonarqube/extensions:/opt/sonarqube/extensions \
  --restart unless-stopped \
  sonarqube:lts-community
```

**Persisted Directories:**
1. `/opt/sonarqube/data` - Database, analysis results, user data
2. `/opt/sonarqube/logs` - Application and access logs
3. `/opt/sonarqube/extensions` - Plugins and custom configurations

**Benefits:**
- âœ… Data survives container restarts
- âœ… No need to reconfigure after restarts
- âœ… Preserves authentication tokens
- âœ… Maintains webhook configurations
- âœ… Retains historical analysis data

**Verification:**
```bash
# Check volume mounts
docker inspect sonar --format='{{range .Mounts}}{{.Source}} -> {{.Destination}}{{println}}{{end}}'

# Verify data persistence
ls -lh /opt/sonarqube/data/
```

---

## 4. SonarQube Docker Restart Policy

### Problem
SonarQube container did not automatically restart after:
- Docker daemon restarts
- EC2 instance reboots
- Manual stops for maintenance

This required manual intervention to bring SonarQube back online.

### Solution Implemented
**Added restart policy to SonarQube container**

**Configuration:**
```bash
docker run -d \
  --restart unless-stopped \
  ...
  sonarqube:lts-community
```

**Restart Policy Options:**
- `no` - Never restart (default)
- `on-failure` - Restart only on failure
- `always` - Always restart (even after manual stop)
- `unless-stopped` - **[CHOSEN]** Restart unless explicitly stopped

**Why `unless-stopped`?**
- Automatic recovery from crashes
- Survives Docker daemon restarts
- Survives EC2 reboots
- But respects manual `docker stop` commands (doesn't restart if we intentionally stop it)

**Benefits:**
- âœ… Automatic recovery from crashes
- âœ… No manual intervention needed after reboots
- âœ… Higher availability
- âœ… Still allows manual control when needed

---

## 5. ALB Health Check Configuration Fix

### Problem
AWS Application Load Balancer (ALB) target groups showed all backend targets as "unhealthy" despite:
- All pods running successfully
- Backend application responding correctly
- MongoDB connection established
- Health endpoints (`/healthz`, `/ready`) working when tested directly

### Root Cause Analysis
**ALB was checking the wrong health check path:**
- ALB Target Group: Health check path = `/` (root)
- Backend Application: Root path `/` not implemented (returns 404)
- Available health endpoints: `/healthz`, `/ready`, `/started`

**Investigation Commands:**
```bash
# Showed health check path was "/"
aws elbv2 describe-target-groups \
  --target-group-arns arn:aws:elasticloadbalancing:us-east-1:296062548155:targetgroup/k8s-threetie-api-e31d5d9fe0/f146db41ea354e20

# Confirmed /healthz works inside pod
kubectl exec -it api-7db88f847c-dgc6f -n three-tier -- curl http://localhost:3500/healthz
# Response: "Healthy" (200 OK)

# Confirmed / returns 404
kubectl exec -it api-7db88f847c-dgc6f -n three-tier -- curl http://localhost:3500/
# Response: "Cannot GET /" (404 Not Found)
```

### Solution Implemented
**Added ALB health check annotations to ingress.yaml**

**Configuration Changes:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mainlb
  namespace: three-tier
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: /healthz
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
```

**Annotation Details:**
| Annotation | Value | Purpose |
|------------|-------|---------|
| healthcheck-path | `/healthz` | Endpoint to check (returns "Healthy") |
| healthcheck-protocol | `HTTP` | Use HTTP for health checks |
| healthcheck-port | `traffic-port` | Same port as application traffic (3500) |
| healthcheck-interval-seconds | `15` | Check every 15 seconds |
| healthcheck-timeout-seconds | `5` | Wait max 5 seconds for response |
| healthy-threshold-count | `2` | 2 consecutive successes = healthy |
| unhealthy-threshold-count | `2` | 2 consecutive failures = unhealthy |

**Application:**
```bash
kubectl apply -f Kubernetes-Manifests-file/ingress.yaml
```

**Verification:**
```bash
# Confirmed health check path updated to /healthz
aws elbv2 describe-target-groups \
  --target-group-arns arn:aws:elasticloadbalancing:us-east-1:296062548155:targetgroup/k8s-threetie-api-e31d5d9fe0/f146db41ea354e20 \
  --query 'TargetGroups[0].[HealthCheckPath,HealthCheckProtocol]'

# Output:
# | /healthz |
# | HTTP     |

# Verified targets became healthy
aws elbv2 describe-target-health \
  --target-group-arn arn:aws:elasticloadbalancing:us-east-1:296062548155:targetgroup/k8s-threetie-api-e31d5d9fe0/f146db41ea354e20

# Output:
# 192.168.27.6   -> healthy
# 192.168.45.166 -> healthy
```

**Results:**
- âœ… Backend targets: **2/2 Healthy** (was 0/2 unhealthy)
- âœ… Health checks passing consistently
- âœ… ALB routing traffic to backend pods

---

## 6. 504 Gateway Timeout Resolution

### Problem
Accessing the application URL resulted in **504 Gateway Timeout** errors:
```
http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/
```

**Error Message:**
```
504 Gateway Timeout
The server didn't respond in time.
```

### Root Cause
The 504 error was a **direct consequence** of the unhealthy backend targets:
1. ALB had no healthy targets to route traffic to
2. All backend pods marked as "unhealthy" (failing health checks)
3. ALB couldn't forward requests â†’ timeout
4. Health check misconfiguration (checking `/` instead of `/healthz`)

### Solution
**Resolved by fixing ALB health check configuration** (see section 5 above)

**Resolution Flow:**
1. Updated ingress.yaml with correct health check annotations
2. Applied ingress changes: `kubectl apply -f Kubernetes-Manifests-file/ingress.yaml`
3. ALB controller detected annotation changes
4. ALB target group health check updated from `/` to `/healthz`
5. Health checks started passing (within 30 seconds)
6. Targets marked as "healthy"
7. ALB started routing traffic successfully
8. **504 errors resolved immediately**

### Verification Steps

**1. Frontend Accessible:**
```bash
curl http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/
```
**Response:** Full TO-DO App HTML (React application)

**2. Backend Health Check:**
```bash
curl http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/healthz
```
**Response:** "Healthy" (200 OK)

**3. Backend API:**
```bash
curl http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/api/tasks
```
**Response:** JSON array of tasks from MongoDB

**4. AWS Console Verification:**
- Target Group `k8s-threetie-api-e31d5d9fe0`: **2 Healthy targets**
- Target Group `k8s-threetie-frontend-33a148363f`: **2 Healthy targets**
- Load Balancer state: **Active**

### Final Application Status
âœ… **Application fully operational:**
- Frontend: Serving React TO-DO application
- Backend: API endpoints responding correctly
- Database: MongoDB connected and operational
- Load Balancer: All targets healthy, routing traffic
- Health Checks: Passing consistently

---

## Summary of Changes

### Infrastructure Changes
| Component | Change | Impact |
|-----------|--------|--------|
| Jenkins EC2 | t2.2xlarge â†’ c6a.2xlarge | Better performance, 18% cost savings |
| Jenkins JVM | Optimized heap & GC settings | Improved stability and responsiveness |
| SonarQube | Added persistent volumes | Data survives container restarts |
| SonarQube | Added restart policy | Automatic recovery from crashes |
| ALB Ingress | Added health check annotations | Fixed target health detection |

### Issues Resolved
1. âœ… Jenkins performance issues during concurrent builds
2. âœ… SonarQube data loss on container restart
3. âœ… SonarQube manual restart required after reboots
4. âœ… ALB backend targets showing unhealthy
5. âœ… 504 Gateway Timeout errors on application access
6. âœ… Application now fully accessible via ALB

### Cost Impact
- **Jenkins Instance:** -$2.20/day (18% reduction)
- **Total Monthly Savings:** ~$66/month
- **Performance:** Improved (better CPU, faster network)

---

## Testing and Validation

### Application Access
```bash
# Main application URL
http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/

# Health endpoints
http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/healthz
http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/ready

# API endpoints
http://k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com/api/tasks
```

### Kubernetes Resources Status
```bash
# All pods running
kubectl get pods -n three-tier
# api-7db88f847c-dgc6f        1/1  Running
# api-7db88f847c-xk9n2        1/1  Running
# frontend-85f5d8c7fb-7j2ks   1/1  Running
# mongodb-6b8b9f8d5c-4x7ks    1/1  Running

# Ingress configured correctly
kubectl get ingress -n three-tier
# mainlb  alb  *  k8s-threetie-mainlb-1dd958d0ec-128970382.us-east-1.elb.amazonaws.com

# Services operational
kubectl get svc -n three-tier
# api         ClusterIP  10.100.235.105  3500/TCP
# frontend    ClusterIP  10.100.41.203   3000/TCP
# mongodb-svc ClusterIP  10.100.128.92   27017/TCP
```

### ALB Target Health
```bash
# Backend targets: 2/2 Healthy
# Frontend targets: 2/2 Healthy
# Total healthy targets: 4/4
```

---

## Lessons Learned

### 1. Health Check Configuration is Critical
- Always explicitly configure health check paths in ingress annotations
- Default health checks (`/`) often don't match application endpoints
- Test health endpoints directly before configuring ALB

### 2. Persistent Storage for Stateful Containers
- Never run stateful applications (databases, SonarQube) without volumes
- Docker volumes are simple and effective for single-host deployments
- Plan backup strategy for persistent volumes

### 3. Instance Type Selection
- Compute-optimized instances (c-family) better for CI/CD workloads
- Don't always assume "bigger is better" - optimize for workload
- Network performance matters for Docker registry operations

### 4. Monitoring and Debugging
- Check application logs first: `kubectl logs`
- Verify health endpoints work: `kubectl exec` + `curl`
- Compare AWS console with Kubernetes state
- Use AWS CLI to verify ALB configuration

### 5. Documentation is Key
- Document infrastructure changes immediately
- Include "why" not just "what" and "how"
- Future troubleshooting depends on good documentation

---

## Related Documentation
- [DOCUMENTATION.md](./DOCUMENTATION.md) - Complete project implementation guide
- [AWS-COST-MANAGEMENT.md](./AWS-COST-MANAGEMENT.md) - Cost optimization strategies
- [NODE-GROUP-RECREATION-GUIDE.md](./NODE-GROUP-RECREATION-GUIDE.md) - EKS node group management
- [JENKINS-INSTANCE-CHANGES.md](./JENKINS-INSTANCE-CHANGES.md) - Detailed Jenkins changes
- [SONARQUBE-FIX-GUIDE.md](./SONARQUBE-FIX-GUIDE.md) - SonarQube troubleshooting
- [JENKINS-PERFORMANCE-FIX.md](./JENKINS-PERFORMANCE-FIX.md) - Performance optimization details

---

## Next Steps
1. âœ… **[COMPLETED]** All critical issues resolved
2. ðŸ”„ **[NEXT]** Set up ArgoCD Image Updater for automated deployments
3. ðŸ“‹ **[PLANNED]** Implement HTTPS with ACM certificate and custom domain
4. ðŸ“‹ **[PLANNED]** Configure Prometheus/Grafana monitoring
5. ðŸ“‹ **[PLANNED]** Implement automated backup for SonarQube data

---

**Document Maintainer:** DevSecOps Team  
**Last Updated:** November 17, 2025  
**Status:** Active - All issues resolved, application operational
